name: Prompt A/B Testing

on:
  pull_request:
    types: [opened, synchronize]
    paths:
      - '**/*.md'
      - '**/*prompt*'
      - '.claude-ab-tests/**'

permissions:
  contents: read
  pull-requests: write
  statuses: write

jobs:
  ab-test:
    runs-on: ubuntu-latest
    if: github.event.pull_request.draft == false
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Clone Expert Bank
        run: |
          git clone https://github.com/whichguy/prompt-expert-bank.git /tmp/expert-bank
      
      - name: Run A/B Test with Claude
        id: test
        uses: anthropics/claude-code-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          model: claude-3-5-sonnet-20241022
          max_turns: 3
          timeout_minutes: 10
          allowed_tools: |
            - str_replace_editor
          prompt: |
            You are conducting a Result-Focused Prompt A/B Test.
            
            ## Your Mission:
            1. Identify changed prompt files using git diff
            2. Load appropriate expert from /tmp/expert-bank/prompt-experts/
            3. Run OLD vs NEW prompts through test scenarios
            4. Evaluate OUTPUTS using expert framework
            5. Write results to .claude-ab-tests/test-results/report.md
            
            ## Process:
            
            1. First, identify what changed:
               git diff origin/${{ github.base_ref }}...HEAD --name-only | grep -E '\.(md|prompt|txt)$'
            
            2. Based on file content, select expert:
               - Security/Command terms ‚Üí security_command_expert.md
               - Financial terms ‚Üí financial_analysis_expert.md
               - GAS/Script terms ‚Üí gas_javascript_expert.md
               - Data/Analysis ‚Üí data_analysis_expert.md
               - Default ‚Üí general_purpose_expert.md
            
            3. For security prompts, use test cases from test-cases.md if available
            
            4. Run both OLD and NEW prompts through scenarios
            
            5. Score outputs using expert's weighted competencies
            
            6. Generate report with:
               - Overall recommendation (APPROVE/REJECT/NEEDS_REVISION)
               - Competency scores with specific examples
               - Key improvements and issues
               - Actionable feedback
            
            Focus on OUTPUT QUALITY, not prompt structure!
      
      - name: Post Results to PR
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const reportPath = '.claude-ab-tests/test-results/report.md';
            
            if (fs.existsSync(reportPath)) {
              const report = fs.readFileSync(reportPath, 'utf8');
              
              // Extract key info
              const rec = report.match(/Recommendation:\s*\*?\*?([^*\n]+)/)?.[1]?.trim() || 'UNKNOWN';
              const confidence = report.match(/Confidence:\s*([^\n]+)/)?.[1] || 'N/A';
              
              // Create concise comment
              let comment = `## üß™ A/B Test Results\n\n`;
              comment += `**Recommendation:** ${rec}\n`;
              comment += `**Confidence:** ${confidence}\n\n`;
              
              if (rec.includes('APPROVE')) {
                comment += `‚úÖ This prompt change improves output quality and is ready to merge.\n\n`;
              } else if (rec.includes('REJECT')) {
                comment += `‚ùå This prompt change reduces output quality and should not be merged.\n\n`;
              } else {
                comment += `‚ö†Ô∏è This prompt change needs revision before merging.\n\n`;
              }
              
              // Add summary
              const summary = report.match(/## Summary\n([^#]+)/)?.[1]?.trim() || 
                             report.substring(0, 500) + '...';
              comment += summary + '\n\n';
              
              comment += `[üìÑ View Full Report](https://github.com/${{ github.repository }}/pull/${{ github.event.pull_request.number }}/files)\n`;
              
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.name,
                body: comment
              });
              
              // Set status check
              const state = rec.includes('APPROVE') ? 'success' : 
                           rec.includes('REJECT') ? 'failure' : 'pending';
              
              await github.rest.repos.createCommitStatus({
                owner: context.repo.owner,
                repo: context.repo.name,
                sha: context.sha,
                state: state,
                description: `Prompt Quality: ${rec}`,
                context: 'prompt-ab-test'
              });
            }
      
      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ab-test-results
          path: .claude-ab-tests/test-results/